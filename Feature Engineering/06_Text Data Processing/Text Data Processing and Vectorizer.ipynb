{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cfd5b6",
   "metadata": {
    "id": "49cfd5b6"
   },
   "source": [
    "# Stemming in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2358d36e",
   "metadata": {
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1729526863808,
     "user": {
      "displayName": "Angkur Dhar",
      "userId": "03411881654721835394"
     },
     "user_tz": -360
    },
    "id": "2358d36e"
   },
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23e6c5a1",
   "metadata": {},
   "source": [
    "üîπ nltk.download('punkt') ‡¶ï‡ßÄ?\n",
    "üî∏ punkt ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø sentence tokenizer model\n",
    "üëâ ‡¶è‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶¨‡ßú text ‡¶ï‡ßá sentence ‡¶¨‡¶æ word ‡¶è ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã‡•§\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b98e81",
   "metadata": {
    "id": "f6b98e81"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9160a67",
   "metadata": {
    "id": "a9160a67",
    "outputId": "90e09556-ee3b-4027-c409-620afd03d28a"
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6a5b84c",
   "metadata": {
    "id": "d6a5b84c"
   },
   "outputs": [],
   "source": [
    "word = ['change','changing','changes','changed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622902d5",
   "metadata": {
    "id": "622902d5",
    "outputId": "31bdf132-f1c0-4414-9336-22c76d4c3d82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change', 'changing', 'changes', 'changed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53806986",
   "metadata": {
    "id": "53806986"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f56cb8d",
   "metadata": {
    "id": "2f56cb8d"
   },
   "outputs": [],
   "source": [
    "p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95280d35",
   "metadata": {
    "id": "95280d35",
    "outputId": "91734004-5bb5-418c-922f-6514bb56e2bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chang'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.stem('change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be1b987",
   "metadata": {
    "id": "5be1b987",
    "outputId": "18d3a2a4-e299-4ce6-d385-1da1a4d9b351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd2eed15",
   "metadata": {
    "id": "bd2eed15",
    "outputId": "d98fbb55-aa09-4e6d-f06f-9d1edb50ecf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change chang\n",
      "changing chang\n",
      "changes chang\n",
      "changed chang\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(w , p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a3d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maind it: ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶π‡ßü Tokenization, ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶π‡ßü Stemming ‡¶¨‡¶æ Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e1eb61e",
   "metadata": {
    "id": "1e1eb61e"
   },
   "outputs": [],
   "source": [
    "sen = 'The constant flux of life necessitates embracing change, whether its adapting to the changes around us or actively changing ourselves to meet new challenges.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f8aeb9e",
   "metadata": {
    "id": "8f8aeb9e",
    "outputId": "999888b8-a8ac-4b8c-9ace-36e6186c2512"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The constant flux of life necessitates embracing change, whether its adapting to the changes around us or actively changing ourselves to meet new challenges.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad049b2c",
   "metadata": {
    "id": "ad049b2c"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4891a15f",
   "metadata": {
    "id": "4891a15f"
   },
   "outputs": [],
   "source": [
    "token = word_tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31e33432",
   "metadata": {
    "id": "31e33432",
    "outputId": "29e13cd0-8c61-44c2-cbe7-82a254a2099a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'constant',\n",
       " 'flux',\n",
       " 'of',\n",
       " 'life',\n",
       " 'necessitates',\n",
       " 'embracing',\n",
       " 'change',\n",
       " ',',\n",
       " 'whether',\n",
       " 'its',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'around',\n",
       " 'us',\n",
       " 'or',\n",
       " 'actively',\n",
       " 'changing',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'new',\n",
       " 'challenges',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18559d9d",
   "metadata": {
    "id": "18559d9d",
    "outputId": "1794d6c6-0a76-4fbd-a476-41042691cd17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'constant',\n",
       " 'flux',\n",
       " 'of',\n",
       " 'life',\n",
       " 'necessitates',\n",
       " 'embracing',\n",
       " 'change,',\n",
       " 'whether',\n",
       " 'its',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'around',\n",
       " 'us',\n",
       " 'or',\n",
       " 'actively',\n",
       " 'changing',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'new',\n",
       " 'challenges.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second way but normal,Not fit for NLP\n",
    "sen.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e9e000f",
   "metadata": {
    "id": "6e9e000f",
    "outputId": "b9b5f820-6532-4265-cf3d-5caace41d154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "constant\n",
      "flux\n",
      "of\n",
      "life\n",
      "necessit\n",
      "embrac\n",
      "chang\n",
      ",\n",
      "whether\n",
      "it\n",
      "adapt\n",
      "to\n",
      "the\n",
      "chang\n",
      "around\n",
      "us\n",
      "or\n",
      "activ\n",
      "chang\n",
      "ourselv\n",
      "to\n",
      "meet\n",
      "new\n",
      "challeng\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in token:\n",
    "    print(p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "347deea7",
   "metadata": {
    "id": "347deea7"
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13a690",
   "metadata": {
    "id": "ca13a690"
   },
   "source": [
    "# Lemmatization in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35256907",
   "metadata": {
    "id": "35256907"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83e06afa",
   "metadata": {
    "id": "83e06afa"
   },
   "outputs": [],
   "source": [
    "le = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b11cd1",
   "metadata": {
    "id": "86b11cd1",
    "outputId": "478a3e8f-63cc-4890-9e3a-e88c55761c9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'constant',\n",
       " 'flux',\n",
       " 'of',\n",
       " 'life',\n",
       " 'necessitates',\n",
       " 'embracing',\n",
       " 'change',\n",
       " ',',\n",
       " 'whether',\n",
       " 'its',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'around',\n",
       " 'us',\n",
       " 'or',\n",
       " 'actively',\n",
       " 'changing',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'new',\n",
       " 'challenges',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cd80d63",
   "metadata": {
    "id": "6cd80d63",
    "outputId": "8f468981-6a07-490e-d346-b3207fb45ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "constant\n",
      "flux\n",
      "of\n",
      "life\n",
      "necessitates\n",
      "embracing\n",
      "change\n",
      ",\n",
      "whether\n",
      "it\n",
      "adapting\n",
      "to\n",
      "the\n",
      "change\n",
      "around\n",
      "u\n",
      "or\n",
      "actively\n",
      "changing\n",
      "ourselves\n",
      "to\n",
      "meet\n",
      "new\n",
      "challenge\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in token:\n",
    "    print(le.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c111d29",
   "metadata": {
    "id": "8c111d29",
    "outputId": "22df7fac-185d-482d-a8f1-109677f59eeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'changing'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lemmatize('changing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1b7921f",
   "metadata": {
    "id": "b1b7921f",
    "outputId": "b9c695bf-8290-453c-a1e0-9d9c151cc50a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'change'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lemmatize('changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62396cd2",
   "metadata": {
    "id": "62396cd2",
    "outputId": "47f9081f-1ad1-498a-8ef2-d220e064d1e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'changed'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lemmatize('changed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f645c9c",
   "metadata": {
    "id": "0f645c9c",
    "outputId": "c90a4c40-70ed-4592-b769-5cdc64947de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change\n",
      "changing\n",
      "change\n",
      "changed\n"
     ]
    }
   ],
   "source": [
    "for i in word:\n",
    "    print(le.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b90bb18f",
   "metadata": {
    "id": "b90bb18f",
    "outputId": "01260a78-1f15-4614-9b98-5363ea39ae5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change', 'changing', 'changes', 'changed']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ecd2a",
   "metadata": {
    "id": "850ecd2a"
   },
   "source": [
    "# Tokenization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc5e77",
   "metadata": {
    "id": "35bc5e77"
   },
   "source": [
    "In Python, there are several libraries and tools available for performing tokenization and other NLP tasks. Here are a few examples using popular libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b22c7e",
   "metadata": {
    "id": "c0b22c7e"
   },
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ecf14",
   "metadata": {
    "id": "2f6ecf14"
   },
   "source": [
    "NLTK (Natural Language Toolkit) is a widely used library for NLP tasks. To perform tokenization using NLTK, you need to install it first. You can do so by running pip install nltk. Here's an example of tokenizing a sentence using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c0265a3",
   "metadata": {
    "id": "2c0265a3",
    "outputId": "77526c27-d219-4713-82ec-a74431aa5b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'from', 'aiQuest', 'Intelligence', '.', 'I', 'am', 'learning', 'NLP', '.', 'It', 'is', 'fascinating', '!']\n",
      "[\"I'm from aiQuest Intelligence.\", 'I am learning NLP.', 'It is fascinating!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "word_tokens = word_tokenize(sentence)\n",
    "sentence_tokens = sent_tokenize(sentence)\n",
    "\n",
    "print(word_tokens)\n",
    "print(sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3707c60",
   "metadata": {
    "id": "a3707c60"
   },
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c406d7",
   "metadata": {
    "id": "f1c406d7"
   },
   "source": [
    "spaCy is another powerful library for NLP. To install spaCy, you can run pip install spacy and then download the appropriate language model. Here's an example of tokenization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b17526b8",
   "metadata": {
    "id": "b17526b8"
   },
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ead2067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f1dc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy.cli\n",
    "#spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22223a05",
   "metadata": {
    "id": "22223a05"
   },
   "source": [
    "[MODEL](https://spacy.io/usage/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42130eca",
   "metadata": {
    "id": "42130eca",
    "outputId": "9e229f79-dbb0-44cf-bd33-5a51733fcc1b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spc = spacy.load('en_core_web_sm')  # Load the English language model\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "doc = spc(sentence)\n",
    "\n",
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32b05b98",
   "metadata": {
    "id": "32b05b98",
    "outputId": "4ee46ca7-9561-4187-896b-edd63f5b9aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'from', 'aiQuest', 'Intelligence', '.', 'I', 'am', 'learning', 'NLP', '.', 'It', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = [token.text for token in doc]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4db8d",
   "metadata": {
    "id": "6ae4db8d"
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7932f5",
   "metadata": {
    "id": "0a7932f5"
   },
   "source": [
    "Transformers is a library built by Hugging Face that provides state-of-the-art pre-trained models for NLP. It offers various functionalities, including tokenization. To install Transformers, run pip install transformers. Here's an example of tokenization using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef052805",
   "metadata": {
    "id": "ef052805",
    "outputId": "213d48b5-d33a-4732-982d-d79f99d7ecc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'from', 'ai', '##quest', 'intelligence', '.', 'i', 'am', 'learning', 'nl', '##p', '.', 'it', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a12b3",
   "metadata": {
    "id": "351a12b3"
   },
   "source": [
    "# Named Entity Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3747ac00",
   "metadata": {
    "id": "3747ac00"
   },
   "source": [
    "To perform named entity tokenization using NLTK (Natural Language Toolkit), you can utilize the named entity recognition (NER) functionality provided by NLTK. Here's an example of how to extract named entity tokens from a sentence using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43e1d062",
   "metadata": {
    "id": "43e1d062"
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('maxent_ne_chunker')  # Download the required resource (NER models)\n",
    "#nltk.download('words')  # Download the required resource (word corpus)\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f061a0c5",
   "metadata": {
    "id": "f061a0c5",
    "outputId": "2943cdcd-27ed-49b0-c09d-68fea34e096a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aiQuest Intelligence', 'NLP', 'Hasan', 'Joe']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!, Hasan khan, my name is Joe\"\n",
    "\n",
    "tokens = word_tokenize(sentence) # Tokenize the sentence into words\n",
    "\n",
    "pos_tags = pos_tag(tokens) # Perform part-of-speech tagging\n",
    "\n",
    "ner_tags = ne_chunk(pos_tags) # Perform named entity recognition\n",
    "\n",
    "named_entity_tokens = []\n",
    "\n",
    "for chunk in ner_tags:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        named_entity_tokens.append(' '.join(c[0] for c in chunk))\n",
    "\n",
    "print(named_entity_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fac1f801",
   "metadata": {
    "id": "fac1f801"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd7cb83e",
   "metadata": {
    "id": "fd7cb83e",
    "outputId": "ca656472-2c96-4b79-cec3-0e6ebd691d3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shakil', 'Lives', 'in', 'Germany']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = \"Shakil Lives in Germany\"\n",
    "tokens = word_tokenize(sentence2)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75627775",
   "metadata": {
    "id": "75627775",
    "outputId": "b3f38b39-355f-44f2-d54a-281e90d34bf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shakil', 'NNP'), ('Lives', 'VBZ'), ('in', 'IN'), ('Germany', 'NNP')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3fd7a",
   "metadata": {
    "id": "ffa3fd7a"
   },
   "source": [
    "# Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "debf601e",
   "metadata": {
    "id": "debf601e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca4bfa72",
   "metadata": {
    "id": "ca4bfa72",
    "outputId": "c569f629-b829-4359-eef3-2d1837fb6f52"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey, I love Bangladesh;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon, I am happy!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I live in Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice to meet you man-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You won an iPhone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  class\n",
       "0      Hey, I love Bangladesh;      1\n",
       "1  Good afternoon, I am happy!      1\n",
       "2            I live in Germany      1\n",
       "3        Nice to meet you man-      1\n",
       "4            You won an iPhone      0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7aa9a9",
   "metadata": {
    "id": "dd7aa9a9"
   },
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "335b48b7",
   "metadata": {
    "id": "335b48b7",
    "outputId": "3a747708-1066-45a7-e012-268d1882c526"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "en_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a18be55d",
   "metadata": {
    "id": "a18be55d",
    "outputId": "eac1f6cb-ad77-4419-f1cf-6e24e943dafa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20250d63",
   "metadata": {
    "id": "20250d63",
    "outputId": "d22f7c95-69da-4a59-b539-57c7de79a3e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡¶Ö‡¶§‡¶è‡¶¨',\n",
       " '‡¶Ö‡¶•‡¶ö',\n",
       " '‡¶Ö‡¶•‡¶¨‡¶æ',\n",
       " '‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ',\n",
       " '‡¶Ö‡¶®‡ßá‡¶ï',\n",
       " '‡¶Ö‡¶®‡ßá‡¶ï‡ßá',\n",
       " '‡¶Ö‡¶®‡ßá‡¶ï‡ßá‡¶á',\n",
       " '‡¶Ö‡¶®‡ßç‡¶§‡¶§',\n",
       " '‡¶Ö‡¶®‡ßç‡¶Ø',\n",
       " '‡¶Ö‡¶¨‡¶ß‡¶ø',\n",
       " '‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø',\n",
       " '‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡¶§',\n",
       " '‡¶Ü‡¶á',\n",
       " '‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ',\n",
       " '‡¶Ü‡¶ó‡ßá',\n",
       " '‡¶Ü‡¶ó‡ßá‡¶á',\n",
       " '‡¶Ü‡¶õ‡ßá',\n",
       " '‡¶Ü‡¶ú',\n",
       " '‡¶Ü‡¶¶‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá',\n",
       " '‡¶Ü‡¶™‡¶®‡¶æ‡¶∞',\n",
       " '‡¶Ü‡¶™‡¶®‡¶ø',\n",
       " '‡¶Ü‡¶¨‡¶æ‡¶∞',\n",
       " '‡¶Ü‡¶Æ‡¶∞‡¶æ',\n",
       " '‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá',\n",
       " '‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞',\n",
       " '‡¶Ü‡¶Æ‡¶æ‡¶∞',\n",
       " '‡¶Ü‡¶Æ‡¶ø',\n",
       " '‡¶Ü‡¶∞',\n",
       " '‡¶Ü‡¶∞‡¶ì',\n",
       " '‡¶á',\n",
       " '‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø',\n",
       " '‡¶á‡¶π‡¶æ',\n",
       " '‡¶â‡¶ö‡¶ø‡¶§',\n",
       " '‡¶â‡¶§‡ßç‡¶§‡¶∞',\n",
       " '‡¶â‡¶®‡¶ø',\n",
       " '‡¶â‡¶™‡¶∞',\n",
       " '‡¶â‡¶™‡¶∞‡ßá',\n",
       " '‡¶è',\n",
       " '‡¶è‡¶Å‡¶¶‡ßá‡¶∞',\n",
       " '‡¶è‡¶Å‡¶∞‡¶æ',\n",
       " '‡¶è‡¶á',\n",
       " '‡¶è‡¶ï‡¶á',\n",
       " '‡¶è‡¶ï‡¶ü‡¶ø',\n",
       " '‡¶è‡¶ï‡¶¨‡¶æ‡¶∞',\n",
       " '‡¶è‡¶ï‡ßá',\n",
       " '‡¶è‡¶ï‡ßç',\n",
       " '‡¶è‡¶ñ‡¶®',\n",
       " '‡¶è‡¶ñ‡¶®‡¶ì',\n",
       " '‡¶è‡¶ñ‡¶æ‡¶®‡ßá',\n",
       " '‡¶è‡¶ñ‡¶æ‡¶®‡ßá‡¶á',\n",
       " '‡¶è‡¶ü‡¶æ',\n",
       " '‡¶è‡¶ü‡¶æ‡¶á',\n",
       " '‡¶è‡¶ü‡¶ø',\n",
       " '‡¶è‡¶§',\n",
       " '‡¶è‡¶§‡¶ü‡¶æ‡¶á',\n",
       " '‡¶è‡¶§‡ßá',\n",
       " '‡¶è‡¶¶‡ßá‡¶∞',\n",
       " '‡¶è‡¶¨',\n",
       " '‡¶è‡¶¨‡¶Ç',\n",
       " '‡¶è‡¶¨‡¶æ‡¶∞',\n",
       " '‡¶è‡¶Æ‡¶®',\n",
       " '‡¶è‡¶Æ‡¶®‡¶ï‡ßÄ',\n",
       " '‡¶è‡¶Æ‡¶®‡¶ø',\n",
       " '‡¶è‡¶∞',\n",
       " '‡¶è‡¶∞‡¶æ',\n",
       " '‡¶è‡¶≤',\n",
       " '‡¶è‡¶∏',\n",
       " '‡¶è‡¶∏‡ßá',\n",
       " '‡¶ê',\n",
       " '‡¶ì',\n",
       " '‡¶ì‡¶Å‡¶¶‡ßá‡¶∞',\n",
       " '‡¶ì‡¶Å‡¶∞',\n",
       " '‡¶ì‡¶Å‡¶∞‡¶æ',\n",
       " '‡¶ì‡¶á',\n",
       " '‡¶ì‡¶ï‡ßá',\n",
       " '‡¶ì‡¶ñ‡¶æ‡¶®‡ßá',\n",
       " '‡¶ì‡¶¶‡ßá‡¶∞',\n",
       " '‡¶ì‡¶∞',\n",
       " '‡¶ì‡¶∞‡¶æ',\n",
       " '‡¶ï‡¶ñ‡¶®‡¶ì',\n",
       " '‡¶ï‡¶§',\n",
       " '‡¶ï‡¶¨‡ßá',\n",
       " '‡¶ï‡¶Æ‡¶®‡ßá',\n",
       " '‡¶ï‡¶Ø‡¶º‡ßá‡¶ï',\n",
       " '‡¶ï‡¶Ø‡¶º‡ßá‡¶ï‡¶ü‡¶ø',\n",
       " '‡¶ï‡¶∞‡¶õ‡ßá',\n",
       " '‡¶ï‡¶∞‡¶õ‡ßá‡¶®',\n",
       " '‡¶ï‡¶∞‡¶§‡ßá',\n",
       " '‡¶ï‡¶∞‡¶¨‡ßá',\n",
       " '‡¶ï‡¶∞‡¶¨‡ßá‡¶®',\n",
       " '‡¶ï‡¶∞‡¶≤‡ßá',\n",
       " '‡¶ï‡¶∞‡¶≤‡ßá‡¶®',\n",
       " '‡¶ï‡¶∞‡¶æ',\n",
       " '‡¶ï‡¶∞‡¶æ‡¶á',\n",
       " '‡¶ï‡¶∞‡¶æ‡¶Ø‡¶º',\n",
       " '‡¶ï‡¶∞‡¶æ‡¶∞',\n",
       " '‡¶ï‡¶∞‡¶ø',\n",
       " '‡¶ï‡¶∞‡¶ø‡¶§‡ßá',\n",
       " '‡¶ï‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ',\n",
       " '‡¶ï‡¶∞‡¶ø‡¶Ø‡¶º‡ßá',\n",
       " '‡¶ï‡¶∞‡ßá',\n",
       " '‡¶ï‡¶∞‡ßá‡¶á',\n",
       " '‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®',\n",
       " '‡¶ï‡¶∞‡ßá‡¶õ‡ßá',\n",
       " '‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®',\n",
       " '‡¶ï‡¶∞‡ßá‡¶®',\n",
       " '‡¶ï‡¶æ‡¶â‡¶ï‡ßá',\n",
       " '‡¶ï‡¶æ‡¶õ',\n",
       " '‡¶ï‡¶æ‡¶õ‡ßá',\n",
       " '‡¶ï‡¶æ‡¶ú',\n",
       " '‡¶ï‡¶æ‡¶ú‡ßá',\n",
       " '‡¶ï‡¶æ‡¶∞‡¶ì',\n",
       " '‡¶ï‡¶æ‡¶∞‡¶£',\n",
       " '‡¶ï‡¶ø',\n",
       " '‡¶ï‡¶ø‡¶Ç‡¶¨‡¶æ',\n",
       " '‡¶ï‡¶ø‡¶õ‡ßÅ',\n",
       " '‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á',\n",
       " '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ',\n",
       " '‡¶ï‡ßÄ',\n",
       " '‡¶ï‡ßá',\n",
       " '‡¶ï‡ßá‡¶â',\n",
       " '‡¶ï‡ßá‡¶â‡¶á',\n",
       " '‡¶ï‡ßá‡¶ñ‡¶æ',\n",
       " '‡¶ï‡ßá‡¶®',\n",
       " '‡¶ï‡ßã‡¶ü‡¶ø',\n",
       " '‡¶ï‡ßã‡¶®',\n",
       " '‡¶ï‡ßã‡¶®‡¶ì',\n",
       " '‡¶ï‡ßã‡¶®‡ßã',\n",
       " '‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá',\n",
       " '‡¶ï‡ßü‡ßá‡¶ï',\n",
       " '‡¶ñ‡ßÅ‡¶¨',\n",
       " '‡¶ó‡¶ø‡¶Ø‡¶º‡ßá',\n",
       " '‡¶ó‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá',\n",
       " '‡¶ó‡¶ø‡ßü‡ßá',\n",
       " '‡¶ó‡ßÅ‡¶≤‡¶ø',\n",
       " '‡¶ó‡ßá‡¶õ‡ßá',\n",
       " '‡¶ó‡ßá‡¶≤',\n",
       " '‡¶ó‡ßá‡¶≤‡ßá',\n",
       " '‡¶ó‡ßã‡¶ü‡¶æ',\n",
       " '‡¶ö‡¶≤‡ßá',\n",
       " '‡¶ö‡¶æ‡¶®',\n",
       " '‡¶ö‡¶æ‡¶Ø‡¶º',\n",
       " '‡¶ö‡¶æ‡¶∞',\n",
       " '‡¶ö‡¶æ‡¶≤‡ßÅ',\n",
       " '‡¶ö‡ßá‡¶Ø‡¶º‡ßá',\n",
       " '‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ',\n",
       " '‡¶õ‡¶æ‡¶°‡¶º‡¶æ',\n",
       " '‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶ì',\n",
       " '‡¶õ‡¶ø‡¶≤',\n",
       " '‡¶õ‡¶ø‡¶≤‡ßá‡¶®',\n",
       " '‡¶ú‡¶®',\n",
       " '‡¶ú‡¶®‡¶ï‡ßá',\n",
       " '‡¶ú‡¶®‡ßá‡¶∞',\n",
       " '‡¶ú‡¶®‡ßç‡¶Ø',\n",
       " '‡¶ú‡¶®‡ßç‡¶Ø‡¶ì‡¶ú‡ßá',\n",
       " '‡¶ú‡¶æ‡¶®‡¶§‡ßá',\n",
       " '‡¶ú‡¶æ‡¶®‡¶æ',\n",
       " '‡¶ú‡¶æ‡¶®‡¶æ‡¶®‡ßã',\n",
       " '‡¶ú‡¶æ‡¶®‡¶æ‡¶Ø‡¶º',\n",
       " '‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá',\n",
       " '‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá',\n",
       " '‡¶ú‡ßá',\n",
       " '‡¶ú‡ßç‡¶®‡¶ú‡¶®',\n",
       " '‡¶ü‡¶ø',\n",
       " '‡¶†‡¶ø‡¶ï',\n",
       " '‡¶§‡¶ñ‡¶®',\n",
       " '‡¶§‡¶§',\n",
       " '‡¶§‡¶•‡¶æ',\n",
       " '‡¶§‡¶¨‡ßÅ',\n",
       " '‡¶§‡¶¨‡ßá',\n",
       " '‡¶§‡¶æ',\n",
       " '‡¶§‡¶æ‡¶Å‡¶ï‡ßá',\n",
       " '‡¶§‡¶æ‡¶Å‡¶¶‡ßá‡¶∞',\n",
       " '‡¶§‡¶æ‡¶Å‡¶∞',\n",
       " '‡¶§‡¶æ‡¶Å‡¶∞‡¶æ',\n",
       " '‡¶§‡¶æ‡¶Å‡¶æ‡¶π‡¶æ‡¶∞‡¶æ',\n",
       " '‡¶§‡¶æ‡¶á',\n",
       " '‡¶§‡¶æ‡¶ì',\n",
       " '‡¶§‡¶æ‡¶ï‡ßá',\n",
       " '‡¶§‡¶æ‡¶§‡ßá',\n",
       " '‡¶§‡¶æ‡¶¶‡ßá‡¶∞',\n",
       " '‡¶§‡¶æ‡¶∞',\n",
       " '‡¶§‡¶æ‡¶∞‡¶™‡¶∞',\n",
       " '‡¶§‡¶æ‡¶∞‡¶æ',\n",
       " '‡¶§‡¶æ‡¶∞‡ßà',\n",
       " '‡¶§‡¶æ‡¶π‡¶≤‡ßá',\n",
       " '‡¶§‡¶æ‡¶π‡¶æ',\n",
       " '‡¶§‡¶æ‡¶π‡¶æ‡¶§‡ßá',\n",
       " '‡¶§‡¶æ‡¶π‡¶æ‡¶∞',\n",
       " '‡¶§‡¶ø‡¶®‡¶ê',\n",
       " '‡¶§‡¶ø‡¶®‡¶ø',\n",
       " '‡¶§‡¶ø‡¶®‡¶ø‡¶ì',\n",
       " '‡¶§‡ßÅ‡¶Æ‡¶ø',\n",
       " '‡¶§‡ßÅ‡¶≤‡ßá',\n",
       " '‡¶§‡ßá‡¶Æ‡¶®',\n",
       " '‡¶§‡ßã',\n",
       " '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞',\n",
       " '‡¶•‡¶æ‡¶ï‡¶¨‡ßá',\n",
       " '‡¶•‡¶æ‡¶ï‡¶¨‡ßá‡¶®',\n",
       " '‡¶•‡¶æ‡¶ï‡¶æ',\n",
       " '‡¶•‡¶æ‡¶ï‡¶æ‡¶Ø‡¶º',\n",
       " '‡¶•‡¶æ‡¶ï‡ßá',\n",
       " '‡¶•‡¶æ‡¶ï‡ßá‡¶®',\n",
       " '‡¶•‡ßá‡¶ï‡ßá',\n",
       " '‡¶•‡ßá‡¶ï‡ßá‡¶á',\n",
       " '‡¶•‡ßá‡¶ï‡ßá‡¶ì',\n",
       " '‡¶¶‡¶ø‡¶ï‡ßá',\n",
       " '‡¶¶‡¶ø‡¶§‡ßá',\n",
       " '‡¶¶‡¶ø‡¶®',\n",
       " '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá',\n",
       " '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá',\n",
       " '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®',\n",
       " '‡¶¶‡¶ø‡¶≤‡ßá‡¶®',\n",
       " '‡¶¶‡ßÅ',\n",
       " '‡¶¶‡ßÅ‡¶á',\n",
       " '‡¶¶‡ßÅ‡¶ü‡¶ø',\n",
       " '‡¶¶‡ßÅ‡¶ü‡ßã',\n",
       " '‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ',\n",
       " '‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞',\n",
       " '‡¶¶‡ßá‡¶ì‡ßü‡¶æ',\n",
       " '‡¶¶‡ßá‡¶ñ‡¶§‡ßá',\n",
       " '‡¶¶‡ßá‡¶ñ‡¶æ',\n",
       " '‡¶¶‡ßá‡¶ñ‡ßá',\n",
       " '‡¶¶‡ßá‡¶®',\n",
       " '‡¶¶‡ßá‡¶Ø‡¶º',\n",
       " '‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ',\n",
       " '‡¶ß‡¶∞‡¶æ',\n",
       " '‡¶ß‡¶∞‡ßá',\n",
       " '‡¶ß‡¶æ‡¶Æ‡¶æ‡¶∞',\n",
       " '‡¶®‡¶§‡ßÅ‡¶®',\n",
       " '‡¶®‡¶Ø‡¶º',\n",
       " '‡¶®‡¶æ',\n",
       " '‡¶®‡¶æ‡¶á',\n",
       " '‡¶®‡¶æ‡¶ï‡¶ø',\n",
       " '‡¶®‡¶æ‡¶ó‡¶æ‡¶¶',\n",
       " '‡¶®‡¶æ‡¶®‡¶æ',\n",
       " '‡¶®‡¶ø‡¶ú‡ßá',\n",
       " '‡¶®‡¶ø‡¶ú‡ßá‡¶á',\n",
       " '‡¶®‡¶ø‡¶ú‡ßá‡¶¶‡ßá‡¶∞',\n",
       " '‡¶®‡¶ø‡¶ú‡ßá‡¶∞',\n",
       " '‡¶®‡¶ø‡¶§‡ßá',\n",
       " '‡¶®‡¶ø‡¶Ø‡¶º‡ßá',\n",
       " '‡¶®‡¶ø‡ßü‡ßá',\n",
       " '‡¶®‡ßá‡¶á',\n",
       " '‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ',\n",
       " '‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞',\n",
       " '‡¶®‡ßá‡¶ì‡ßü‡¶æ',\n",
       " '‡¶®‡ßü',\n",
       " '‡¶™‡¶ï‡ßç‡¶∑‡ßá',\n",
       " '‡¶™‡¶∞',\n",
       " '‡¶™‡¶∞‡ßá',\n",
       " '‡¶™‡¶∞‡ßá‡¶á',\n",
       " '‡¶™‡¶∞‡ßá‡¶ì',\n",
       " '‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§',\n",
       " '‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ',\n",
       " '‡¶™‡¶æ‡¶ö',\n",
       " '‡¶™‡¶æ‡¶∞‡¶ø',\n",
       " '‡¶™‡¶æ‡¶∞‡ßá',\n",
       " '‡¶™‡¶æ‡¶∞‡ßá‡¶®',\n",
       " '‡¶™‡¶ø',\n",
       " '‡¶™‡ßá‡¶Ø‡¶º‡ßá',\n",
       " '‡¶™‡ßá‡ßü‡ßç‡¶∞‡ßç',\n",
       " '‡¶™‡ßç‡¶∞‡¶§‡¶ø',\n",
       " '‡¶™‡ßç‡¶∞‡¶•‡¶Æ',\n",
       " '‡¶™‡ßç‡¶∞‡¶≠‡ßÉ‡¶§‡¶ø',\n",
       " '‡¶™‡ßç‡¶∞‡¶Ø‡¶®‡ßç‡¶§',\n",
       " '‡¶™‡ßç‡¶∞‡¶æ‡¶•‡¶Æ‡¶ø‡¶ï',\n",
       " '‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º',\n",
       " '‡¶™‡ßç‡¶∞‡¶æ‡ßü',\n",
       " '‡¶´‡¶≤‡ßá',\n",
       " '‡¶´‡¶ø‡¶∞‡ßá',\n",
       " '‡¶´‡ßá‡¶∞',\n",
       " '‡¶¨‡¶ï‡ßç‡¶§‡¶¨‡ßç‡¶Ø',\n",
       " '‡¶¨‡¶¶‡¶≤‡ßá',\n",
       " '‡¶¨‡¶®',\n",
       " '‡¶¨‡¶∞‡¶Ç',\n",
       " '‡¶¨‡¶≤‡¶§‡ßá',\n",
       " '‡¶¨‡¶≤‡¶≤',\n",
       " '‡¶¨‡¶≤‡¶≤‡ßá‡¶®',\n",
       " '‡¶¨‡¶≤‡¶æ',\n",
       " '‡¶¨‡¶≤‡ßá',\n",
       " '‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®',\n",
       " '‡¶¨‡¶≤‡ßá‡¶®',\n",
       " '‡¶¨‡¶∏‡ßá',\n",
       " '‡¶¨‡¶π‡ßÅ',\n",
       " '‡¶¨‡¶æ',\n",
       " '‡¶¨‡¶æ‡¶¶‡ßá',\n",
       " '‡¶¨‡¶æ‡¶∞',\n",
       " '‡¶¨‡¶ø',\n",
       " '‡¶¨‡¶ø‡¶®‡¶æ',\n",
       " '‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶®',\n",
       " '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑',\n",
       " '‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ü‡¶ø',\n",
       " '‡¶¨‡ßá‡¶∂',\n",
       " '‡¶¨‡ßá‡¶∂‡¶ø',\n",
       " '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞',\n",
       " '‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶∞‡ßá',\n",
       " '‡¶≠‡¶æ‡¶¨‡ßá',\n",
       " '‡¶≠‡¶æ‡¶¨‡ßá‡¶á',\n",
       " '‡¶Æ‡¶§‡ßã',\n",
       " '‡¶Æ‡¶§‡ßã‡¶á',\n",
       " '‡¶Æ‡¶ß‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá',\n",
       " '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá',\n",
       " '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶á',\n",
       " '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶ì',\n",
       " '‡¶Æ‡¶®‡ßá',\n",
       " '‡¶Æ‡¶æ‡¶§‡ßç‡¶∞',\n",
       " '‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá',\n",
       " '‡¶Æ‡ßã‡¶ü',\n",
       " '‡¶Æ‡ßã‡¶ü‡ßá‡¶á',\n",
       " '‡¶Ø‡¶ñ‡¶®',\n",
       " '‡¶Ø‡¶§',\n",
       " '‡¶Ø‡¶§‡¶ü‡¶æ',\n",
       " '‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü',\n",
       " '‡¶Ø‡¶¶‡¶ø',\n",
       " '‡¶Ø‡¶¶‡¶ø‡¶ì',\n",
       " '‡¶Ø‡¶æ',\n",
       " '‡¶Ø‡¶æ‡¶Å‡¶∞',\n",
       " '‡¶Ø‡¶æ‡¶Å‡¶∞‡¶æ',\n",
       " '‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ',\n",
       " '‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞',\n",
       " '‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ',\n",
       " '‡¶Ø‡¶æ‡¶ï‡ßá',\n",
       " '‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá',\n",
       " '‡¶Ø‡¶æ‡¶§‡ßá',\n",
       " '‡¶Ø‡¶æ‡¶¶‡ßá‡¶∞',\n",
       " '‡¶Ø‡¶æ‡¶®',\n",
       " '‡¶Ø‡¶æ‡¶¨‡ßá',\n",
       " '‡¶Ø‡¶æ‡¶Ø‡¶º',\n",
       " '‡¶Ø‡¶æ‡¶∞',\n",
       " '‡¶Ø‡¶æ‡¶∞‡¶æ',\n",
       " '‡¶Ø‡¶ø‡¶®‡¶ø',\n",
       " '‡¶Ø‡ßá',\n",
       " '‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá',\n",
       " '‡¶Ø‡ßá‡¶§‡ßá',\n",
       " '‡¶Ø‡ßá‡¶®',\n",
       " '‡¶Ø‡ßá‡¶Æ‡¶®',\n",
       " '‡¶∞',\n",
       " '‡¶∞‡¶ï‡¶Æ',\n",
       " '‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá',\n",
       " '‡¶∞‡¶æ‡¶ñ‡¶æ',\n",
       " '‡¶∞‡ßá‡¶ñ‡ßá',\n",
       " '‡¶≤‡¶ï‡ßç‡¶∑',\n",
       " '‡¶∂‡ßÅ‡¶ß‡ßÅ',\n",
       " '‡¶∂‡ßÅ‡¶∞‡ßÅ',\n",
       " '‡¶∏‡¶ô‡ßç‡¶ó‡ßá',\n",
       " '‡¶∏‡¶ô‡ßç‡¶ó‡ßá‡¶ì',\n",
       " '‡¶∏‡¶¨',\n",
       " '‡¶∏‡¶¨‡¶æ‡¶∞',\n",
       " '‡¶∏‡¶Æ‡¶∏‡ßç‡¶§',\n",
       " '‡¶∏‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø',\n",
       " '‡¶∏‡¶π',\n",
       " '‡¶∏‡¶π‡¶ø‡¶§',\n",
       " '‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£',\n",
       " '‡¶∏‡¶æ‡¶Æ‡¶®‡ßá',\n",
       " '‡¶∏‡¶ø',\n",
       " '‡¶∏‡ßÅ‡¶§‡¶∞‡¶æ‡¶Ç',\n",
       " '‡¶∏‡ßá',\n",
       " '‡¶∏‡ßá‡¶á',\n",
       " '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®',\n",
       " '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá',\n",
       " '‡¶∏‡ßá‡¶ü‡¶æ',\n",
       " '‡¶∏‡ßá‡¶ü‡¶æ‡¶á',\n",
       " '‡¶∏‡ßá‡¶ü‡¶æ‡¶ì',\n",
       " '‡¶∏‡ßá‡¶ü‡¶ø',\n",
       " '‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü',\n",
       " '‡¶∏‡ßç‡¶¨‡¶Ø‡¶º‡¶Ç',\n",
       " '‡¶π‡¶á‡¶§‡ßá',\n",
       " '‡¶π‡¶á‡¶¨‡ßá',\n",
       " '‡¶π‡¶á‡¶Ø‡¶º‡¶æ',\n",
       " '‡¶π‡¶ì‡¶Ø‡¶º‡¶æ',\n",
       " '‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º',\n",
       " '‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞',\n",
       " '‡¶π‡¶ö‡ßç‡¶õ‡ßá',\n",
       " '‡¶π‡¶§',\n",
       " '‡¶π‡¶§‡ßá',\n",
       " '‡¶π‡¶§‡ßá‡¶á',\n",
       " '‡¶π‡¶®',\n",
       " '‡¶π‡¶¨‡ßá',\n",
       " '‡¶π‡¶¨‡ßá‡¶®',\n",
       " '‡¶π‡¶Ø‡¶º',\n",
       " '‡¶π‡¶Ø‡¶º‡¶§‡ßã',\n",
       " '‡¶π‡¶Ø‡¶º‡¶®‡¶ø',\n",
       " '‡¶π‡¶Ø‡¶º‡ßá',\n",
       " '‡¶π‡¶Ø‡¶º‡ßá‡¶á',\n",
       " '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤',\n",
       " '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá',\n",
       " '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®',\n",
       " '‡¶π‡¶≤',\n",
       " '‡¶π‡¶≤‡ßá',\n",
       " '‡¶π‡¶≤‡ßá‡¶á',\n",
       " '‡¶π‡¶≤‡ßá‡¶ì',\n",
       " '‡¶π‡¶≤‡ßã',\n",
       " '‡¶π‡¶æ‡¶ú‡¶æ‡¶∞',\n",
       " '‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá',\n",
       " '‡¶π‡ßà‡¶≤‡ßá',\n",
       " '‡¶π‡ßã‡¶ï',\n",
       " '‡¶π‡ßü']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('bengali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1932806f",
   "metadata": {
    "id": "1932806f",
    "outputId": "bc0c5a78-d0f9-4a7a-c827-ed589a251169"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd12b1f4",
   "metadata": {
    "id": "fd12b1f4",
    "outputId": "fc832153-b682-49a5-b3e8-c14570838082"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f194d18",
   "metadata": {
    "id": "5f194d18",
    "outputId": "c321cd61-1773-460a-e7fc-1f3fe97b5a13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shakil'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ShAKil'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf3a9eca",
   "metadata": {
    "id": "bf3a9eca"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    remove_punc = [char for char in text if char not in string.punctuation] # Remove punctuation\n",
    "    clean_words = ''.join(remove_punc) # char joining\n",
    "    split_words = clean_words.split()\n",
    "\n",
    "    #Remove stopwords\n",
    "    text = ([word for word in split_words if word.lower() not in en_stopwords]) # stopword = stopwords.words('english')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca490d3c",
   "metadata": {
    "id": "ca490d3c"
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bea0f44",
   "metadata": {
    "id": "8bea0f44",
    "outputId": "44b54976-a531-418d-8782-4f0ab748a673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Hey, love, Bangladesh]\n",
       "1    [Good, afternoon, happy]\n",
       "2             [live, Germany]\n",
       "3           [Nice, meet, man]\n",
       "4                    [iPhone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0aceadbb",
   "metadata": {
    "id": "0aceadbb",
    "outputId": "017aec84-b2f2-4086-daad-9a4f5e7c93cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Hey, love, Bangladesh]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Good, afternoon, happy]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[live, Germany]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Nice, meet, man]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[iPhone]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text  class\n",
       "0   [Hey, love, Bangladesh]      1\n",
       "1  [Good, afternoon, happy]      1\n",
       "2           [live, Germany]      1\n",
       "3         [Nice, meet, man]      1\n",
       "4                  [iPhone]      0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e811b73f",
   "metadata": {
    "id": "e811b73f"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9602341",
   "metadata": {
    "id": "a9602341",
    "outputId": "4eeb1300-a90e-4b52-af59-d57c77f4307f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey love Bangladesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>live Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice meet man</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  class\n",
       "0   Hey love Bangladesh      1\n",
       "1  Good afternoon happy      1\n",
       "2          live Germany      1\n",
       "3         Nice meet man      1\n",
       "4                iPhone      0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25fbe5",
   "metadata": {
    "id": "4a25fbe5"
   },
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0a3791c",
   "metadata": {
    "id": "f0a3791c"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd44238e",
   "metadata": {
    "id": "dd44238e"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "553a30d6",
   "metadata": {
    "id": "553a30d6",
    "outputId": "fdecd66d-9042-481d-c88c-4e6f32276035"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x = cv.fit_transform(df['text'])\n",
    "cv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "431cff4c",
   "metadata": {
    "id": "431cff4c",
    "outputId": "577436d4-7f8e-4f9d-ca04-561c6f10c52c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "698d461a",
   "metadata": {
    "id": "698d461a",
    "outputId": "271d136e-abeb-4129-cecb-61e5ff040b8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11\n",
       "0   0   1   0   0   0   1   0   0   1   0   0   0\n",
       "1   1   0   0   1   1   0   0   0   0   0   0   0\n",
       "2   0   0   1   0   0   0   0   1   0   0   0   0\n",
       "3   0   0   0   0   0   0   0   0   0   1   1   1\n",
       "4   0   0   0   0   0   0   1   0   0   0   0   0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(cv_x.toarray())\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f79e077d",
   "metadata": {
    "id": "f79e077d",
    "outputId": "2d056981-ecbe-405f-aa45-8418480bf06f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['afternoon', 'bangladesh', 'germany', 'good', 'happy', 'hey',\n",
       "       'iphone', 'live', 'love', 'man', 'meet', 'nice'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "680ed45d",
   "metadata": {
    "id": "680ed45d"
   },
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(cv_x.toarray(), index=df['text'], columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1c27935",
   "metadata": {
    "id": "d1c27935",
    "outputId": "32dff471-f7d7-48cf-b382-f9f6cbb12fd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>germany</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>hey</th>\n",
       "      <th>iphone</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>man</th>\n",
       "      <th>meet</th>\n",
       "      <th>nice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hey love Bangladesh</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good afternoon happy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live Germany</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nice meet man</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPhone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      afternoon  bangladesh  germany  good  happy  hey  \\\n",
       "text                                                                     \n",
       "Hey love Bangladesh           0           1        0     0      0    1   \n",
       "Good afternoon happy          1           0        0     1      1    0   \n",
       "live Germany                  0           0        1     0      0    0   \n",
       "Nice meet man                 0           0        0     0      0    0   \n",
       "iPhone                        0           0        0     0      0    0   \n",
       "\n",
       "                      iphone  live  love  man  meet  nice  \n",
       "text                                                       \n",
       "Hey love Bangladesh        0     0     1    0     0     0  \n",
       "Good afternoon happy       0     0     0    0     0     0  \n",
       "live Germany               0     1     0    0     0     0  \n",
       "Nice meet man              0     0     0    1     1     1  \n",
       "iPhone                     1     0     0    0     0     0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5fd34",
   "metadata": {
    "id": "acd5fd34"
   },
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29a5e96e",
   "metadata": {
    "id": "29a5e96e"
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "374ce011",
   "metadata": {
    "id": "374ce011"
   },
   "outputs": [],
   "source": [
    "tf_z = tf.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db374268",
   "metadata": {
    "id": "db374268",
    "outputId": "d9262f33-14b5-43d8-d578-f6e9649e4702"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x12 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32dc4e59",
   "metadata": {
    "id": "32dc4e59"
   },
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(tf_z.toarray(), index=df['text'], columns=tf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a0e7c7d",
   "metadata": {
    "id": "1a0e7c7d",
    "outputId": "5532aeec-ebed-44ea-a143-7808a6ea9625"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>germany</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>hey</th>\n",
       "      <th>iphone</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>man</th>\n",
       "      <th>meet</th>\n",
       "      <th>nice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hey love Bangladesh</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good afternoon happy</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live Germany</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nice meet man</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPhone</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      afternoon  bangladesh   germany     good    happy  \\\n",
       "text                                                                      \n",
       "Hey love Bangladesh     0.00000     0.57735  0.000000  0.00000  0.00000   \n",
       "Good afternoon happy    0.57735     0.00000  0.000000  0.57735  0.57735   \n",
       "live Germany            0.00000     0.00000  0.707107  0.00000  0.00000   \n",
       "Nice meet man           0.00000     0.00000  0.000000  0.00000  0.00000   \n",
       "iPhone                  0.00000     0.00000  0.000000  0.00000  0.00000   \n",
       "\n",
       "                          hey  iphone      live     love      man     meet  \\\n",
       "text                                                                         \n",
       "Hey love Bangladesh   0.57735     0.0  0.000000  0.57735  0.00000  0.00000   \n",
       "Good afternoon happy  0.00000     0.0  0.000000  0.00000  0.00000  0.00000   \n",
       "live Germany          0.00000     0.0  0.707107  0.00000  0.00000  0.00000   \n",
       "Nice meet man         0.00000     0.0  0.000000  0.00000  0.57735  0.57735   \n",
       "iPhone                0.00000     1.0  0.000000  0.00000  0.00000  0.00000   \n",
       "\n",
       "                         nice  \n",
       "text                           \n",
       "Hey love Bangladesh   0.00000  \n",
       "Good afternoon happy  0.00000  \n",
       "live Germany          0.00000  \n",
       "Nice meet man         0.57735  \n",
       "iPhone                0.00000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69b187",
   "metadata": {
    "id": "1e69b187"
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4EHjFi77-om",
   "metadata": {
    "id": "d4EHjFi77-om"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fb5a9cd",
   "metadata": {
    "id": "6fb5a9cd"
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "327f605e",
   "metadata": {
    "id": "327f605e"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1443e1cb",
   "metadata": {
    "id": "1443e1cb",
    "outputId": "558b52d6-fbd6-42a0-8287-66dee5624590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hey', 'love', 'Bangladesh'],\n",
       " ['Good', 'afternoon', 'happy'],\n",
       " ['live', 'Germany'],\n",
       " ['Nice', 'meet', 'man'],\n",
       " ['iPhone']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector = [nltk.word_tokenize(test) for test in df['text']]\n",
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8daabe52",
   "metadata": {
    "id": "8daabe52"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(text_vector, min_count=1) #shift+tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "715616bf",
   "metadata": {
    "id": "715616bf",
    "outputId": "7037239b-d9b4-492b-c85d-c655202b57c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1edcd0e2d10>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a97a89f",
   "metadata": {
    "id": "9a97a89f",
    "outputId": "7dc19d9b-4112-4760-ec9f-7db3ed0f8d62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meet', 0.1459505707025528),\n",
       " ('love', 0.05048206448554993),\n",
       " ('Nice', 0.041577354073524475),\n",
       " ('Germany', 0.03476494178175926),\n",
       " ('live', 0.01915225386619568),\n",
       " ('iPhone', 0.01613469421863556),\n",
       " ('Good', 0.008826175704598427),\n",
       " ('afternoon', 0.004842504393309355),\n",
       " ('Bangladesh', 0.0019510749261826277),\n",
       " ('Hey', -0.08382604271173477)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a840167",
   "metadata": {
    "id": "9a840167"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
